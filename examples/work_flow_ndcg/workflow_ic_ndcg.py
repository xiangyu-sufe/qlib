#  Copyright (c) Microsoft Corporation.
#  Licensed under the MIT License.
"""
Qlib provides two kinds of interfaces.
(1) Users could define the Quant research workflow by a simple configuration.
(2) Qlib is designed in a modularized way and supports creating research workflow by code just like building blocks.

The interface of (1) is `qrun XXX.yaml`.  The interface of (2) is script like this, which nearly does the same thing as `qrun XXX.yaml`
"""
import qlib
from qlib.constant import REG_CN
from qlib.contrib.report import analysis_position
from qlib.utils import init_instance_by_config
from qlib.utils.hxy_utils import get_label, prepare_task_pool
from torch.utils.data import TensorDataset, DataLoader
import torch
import pandas as pd
import os

if __name__ == "__main__":
    # æ•°æ®å‚æ•°
    import argparse

    parser = argparse.ArgumentParser()

    parser.add_argument("--onlyrun_task_id", type=int, default=None, help="Only run task id")
    parser.add_argument("--onlyrun_seed_id", type=int, default=0, help="Only run specified seed id")
    parser.add_argument("--pv1pv5", type=int, default=1, help="PV1 or PV5 day setting")
    parser.add_argument("--fake", action="store_true", default=False, help="Fake data")
    parser.add_argument("--gpu", type=int, default=0,)
    # æ•°æ®é›†é•¿åº¦å‚æ•°
    parser.add_argument("--train_length", type=int, default=2400, help="Training dataset length")
    parser.add_argument("--valid_length", type=int, default=240, help="Validation dataset length")
    parser.add_argument("--test_length", type=int, default=240, help="Test dataset length")

    # æ—¶é—´èŒƒå›´å‚æ•°
    parser.add_argument("--start_time", type=str, default="2019-12-31", help="Start time for data")
    parser.add_argument("--end_time", type=str, default="2024-12-31", help="End time for data")
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--save_path", type=str, default=".")
    parser.add_argument("--sigma", type=float, default=3.03)
    parser.add_argument("--combine_type", type=str, default="null", ) # éœ€è¦è®¾ç½®
    parser.add_argument("--weight", type=float, default=0.7, )
    parser.add_argument("--step_len", type=int, default=10, )
    args = parser.parse_args()
    save_path = args.save_path
    save_path = os.path.join(save_path, f'seed{args.onlyrun_seed_id}')
    os.makedirs(save_path, exist_ok=True)

    root_dir = os.path.expanduser('~')
    alphamat_path = f'{root_dir}/GRU/alphamat/20250625/data/'
    provider_uri = "~/.qlib/qlib_data/cn_data"  # target_dir
    qlib.init(provider_uri=provider_uri, region=REG_CN)
    market = "csiall"
    benchmark = "SH000905"

    task_config = {
        'train': args.train_length,
        'valid': args.valid_length,
        'test': args.test_length,
    }

    only_run_task_pool = prepare_task_pool(args.onlyrun_task_id,
                                        task_config,
                                        path = alphamat_path,
                                        start_time = args.start_time,
                                        end_time = args.end_time)

    # æ•°æ®å‚æ•°
    infer_processors = [
        {"class": "ProcessInfHXY", "kwargs": {}}, # æ›¿æ¢ä¸º nan
        # {"class": "CSRankNorm", "kwargs": {"fields_group": "feature", 'parallel':True, 'n_jobs': 60}},
        {"class": "RobustZScoreNorm", "kwargs": {"fields_group": "feature",}},
        {"class": "Fillna", 'kwargs': {'fields_group': 'feature'}},
    ]
    
    # æ’åºå­¦ä¹  label ä¸ç”¨å¤„ç†
    learn_processors = [
        {"class": "DropnaLabel"},
    ]

    filter_pipe = [
        {
            "filter_type": "ExpressionDFilter",
            "rule_expression": "$volume > 1e-5",
            "filter_start_time": None,
            "filter_end_time": None,
            "keep": False
        }
    ]
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨ç¼“å­˜
    handler_cache_path = os.path.join(save_path, "handler_cache.pkl")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜
    if os.path.exists(handler_cache_path):
        print(f"âœ… æ‰¾åˆ°æ•°æ®å¤„ç†å™¨ç¼“å­˜: {handler_cache_path}")
        # ä½¿ç”¨ç¼“å­˜çš„å¤„ç†å™¨é…ç½®
        handler_config = f"file://{handler_cache_path}"
    else:
        print("ğŸ”„ åˆ›å»ºæ–°çš„æ•°æ®å¤„ç†å™¨...")
        # ä½¿ç”¨æ•´ä¸ªæ—¶é—´èŒƒå›´åˆ›å»ºå¤„ç†å™¨ï¼Œè¿™æ ·å¯ä»¥è¦†ç›–æ‰€æœ‰ä»»åŠ¡
        data_handler_config = {
            "start_time": args.start_time,
            "end_time": args.end_time,
            "fit_start_time": args.start_time,
            "fit_end_time": args.end_time,
            "instruments": market,
            "infer_processors": infer_processors,
            "learn_processors": learn_processors,
            "drop_raw": True,
            "filter_pipe": filter_pipe,
        }
        
        # åˆ›å»ºå¹¶ä¿å­˜å¤„ç†å™¨
        from qlib.contrib.data.handler import Alpha158
        handler = Alpha158(**data_handler_config)
        handler.to_pickle(handler_cache_path, dump_all=True)
        print(f"âœ… æ•°æ®å¤„ç†å™¨å·²ä¿å­˜åˆ°: {handler_cache_path}")
        handler_config = f"file://{handler_cache_path}"
    
    # æ ¹æ®only_run_task_poolè¿›è¡Œè¿­ä»£
    for task_id, segments in only_run_task_pool.items():
        print(f"å¼€å§‹å¤„ç†ä»»åŠ¡ {task_id}")
        segments = segments['segments']
        # ä½¿ç”¨segmentsä¸­çš„æ—¥æœŸ
        start_time = segments['train'][0]  # è®­ç»ƒå¼€å§‹æ—¥æœŸ
        fit_end_time = segments['train'][1]  # è®­ç»ƒç»“æŸæ—¥æœŸ
        val_start_time = segments['valid'][0]  # éªŒè¯å¼€å§‹æ—¥æœŸ
        val_end_time = segments['valid'][1]  # éªŒè¯ç»“æŸæ—¥æœŸ
        test_start_time = segments['test'][0]  # æµ‹è¯•å¼€å§‹æ—¥æœŸ
        test_end_time = segments['test'][1]  # æµ‹è¯•ç»“æŸæ—¥æœŸ
        
        # çº¢è‰²æ‰“å°æ—¥æœŸä¿¡æ¯
        print(f"\033[31mè®­ç»ƒæ—¥æœŸ: {start_time} - {fit_end_time}\033[0m")
        print(f"\033[31méªŒè¯æ—¥æœŸ: {val_start_time} - {val_end_time}\033[0m")
        print(f"\033[31mæµ‹è¯•æ—¥æœŸ: {test_start_time} - {test_end_time}\033[0m")
        
        print(f"æ—¶é—´èŒƒå›´: è®­ç»ƒ({start_time} - {fit_end_time}), éªŒè¯({val_start_time} - {val_end_time}), æµ‹è¯•({test_start_time} - {test_end_time})")   

        task = {
            "model": {
                "class": "GRUNDCG",
                "module_path": "qlib.contrib.model.pytorch_gru_ts_ic_ndcg",
                "kwargs": {
                    "d_feat": 158,
                    "hidden_size": 64,
                    "num_layers": 2,
                    "dropout": 0.0,
                    "n_epochs": 50,
                    "batch_size": 5000,
                    "lr": args.lr,
                    "early_stop": 10,
                    "metric": "ndcg",
                    "loss": "ic",
                    "n_jobs": 24,
                    "GPU": args.gpu,
                    "seed": args.onlyrun_seed_id,
                    "sigma": args.sigma,
                    "n_layer": 5,
                    "weight": args.weight,
                    "linear_ndcg": False,
                    "combine_type":args.combine_type,
                    "debug": True,  # Set to True for debugging mode
                    "save_path": f"{save_path}/task_{task_id}"
                },
            },
            "dataset": {
                "class": "TSDatasetH",
                "module_path": "qlib.data.dataset",
                "kwargs": {
                    "handler": handler_config,  # ä½¿ç”¨ç¼“å­˜çš„å¤„ç†å™¨
                    "segments": {
                        "train": (start_time, fit_end_time),
                        "valid": (val_start_time, val_end_time),
                        "test": (test_start_time, test_end_time),
                    },
                    "step_len": args.step_len,
                },
            },
        }
        

        # model initialization
        if args.fake:
            model = init_instance_by_config(task["model"])
            print("ä½¿ç”¨å‡æ•°æ®æµ‹è¯• model")
            N, T, D = 5000, 20, 159
            batch_size = 10
            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            features = torch.randn(N, T, D, dtype=torch.float32, device=device)
            # æ ‡ç­¾æ•°æ®: (N,) = (1000,)
            labels = torch.randn(N, dtype=torch.float32, device=device)
            
            # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
            dataset = TensorDataset(features, labels)
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
            model.train_epoch(dataloader)
            model.test_epoch(dataloader)
        else:
            model = init_instance_by_config(task["model"])
            dataset = init_instance_by_config(task["dataset"])
            model.fit(dataset)
            score = model.predict(dataset)
            score.name = 'score'
            # ä¿å­˜é¢„æµ‹å€¼
            score.to_csv(f"{save_path}/task_{task_id}/test.csv")
            score = score.to_frame()
            print(score.head())
            # æµ‹è¯•é›†çš„å›¾
            label = get_label(dataset, segment="test")
            label.columns = ["label"]
            
            pred_label = pd.concat([label, score], axis=1, sort=True).reindex(score.index)
            
            # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
            import os
            os.makedirs(f"{save_path}/task_{task_id}", exist_ok=True)
            
            # ä¿å­˜å›¾ç‰‡ - ä½¿ç”¨å¤šç§æ–¹æ³•
            try:
                fig, = analysis_position.score_ic_graph(pred_label, show_notebook=False)
                # æ–¹æ³•1: å°è¯•ä¿å­˜ä¸ºPNG
                try:
                    fig.write_image(f"{save_path}/task_{task_id}/score_ic_graph.png")
                    print("âœ… æˆåŠŸä¿å­˜ score_ic_graph.png")
                except Exception as png_error:
                    print(f"âš ï¸ PNGä¿å­˜å¤±è´¥: {png_error}")
                    # æ–¹æ³•2: ä¿å­˜ä¸ºHTMLæ–‡ä»¶
                    try:
                        fig.write_html(f"{save_path}/task_{task_id}/score_ic_graph.html")
                        print("âœ… æˆåŠŸä¿å­˜ score_ic_graph.html (å¯åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹)")
                    except Exception as html_error:
                        print(f"âš ï¸ HTMLä¿å­˜å¤±è´¥: {html_error}")
                        # æ–¹æ³•3: ä¿å­˜ä¸ºJSONæ–‡ä»¶
                        try:
                            fig.write_json(f"{save_path}/task_{task_id}/score_ic_graph.json")
                            print("âœ… æˆåŠŸä¿å­˜ score_ic_graph.json (plotlyæ ¼å¼)")
                        except Exception as json_error:
                            print(f"âŒ æ‰€æœ‰ä¿å­˜æ–¹æ³•éƒ½å¤±è´¥: {json_error}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆ score_ic_graph æ—¶å‡ºé”™: {e}")
                
            try:
                fig, = analysis_position.top_score_ic_graph(pred_label, show_notebook=False)
                # æ–¹æ³•1: å°è¯•ä¿å­˜ä¸ºPNG
                try:
                    fig.write_image(f"{save_path}/task_{task_id}/top_score_ic_graph.png")
                    print("âœ… æˆåŠŸä¿å­˜ top_score_ic_graph.png")
                except Exception as png_error:
                    print(f"âš ï¸ PNGä¿å­˜å¤±è´¥: {png_error}")
                    # æ–¹æ³•2: ä¿å­˜ä¸ºHTMLæ–‡ä»¶
                    try:
                        fig.write_html(f"{save_path}/task_{task_id}/top_score_ic_graph.html")
                        print("âœ… æˆåŠŸä¿å­˜ top_score_ic_graph.html (å¯åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹)")
                    except Exception as html_error:
                        print(f"âš ï¸ HTMLä¿å­˜å¤±è´¥: {html_error}")
                        # æ–¹æ³•3: ä¿å­˜ä¸ºJSONæ–‡ä»¶
                        try:
                            fig.write_json(f"{save_path}/task_{task_id}/top_score_ic_graph.json")
                            print("âœ… æˆåŠŸä¿å­˜ top_score_ic_graph.json (plotlyæ ¼å¼)")
                        except Exception as json_error:
                            print(f"âŒ æ‰€æœ‰ä¿å­˜æ–¹æ³•éƒ½å¤±è´¥: {json_error}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆ top_score_ic_graph æ—¶å‡ºé”™: {e}")
            
            print(f"ä»»åŠ¡ {task_id} å®Œæˆ")